{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get city code and folder name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# original filename: nn_baseline2.ipynb\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import logging\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocess data and train/test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['AN_MJ', 'WE_WJ', 'AR_MA', 'HC_WJ', 'AL_MJ', 'JS_WA', 'SL_MJ', 'PE_WA', 'BD_MA', 'SU_WJ', 'BX_MJ', 'WD_WA', 'FO_MJ', 'ME_WA', 'AP_MA', 'EZ_WA']\n",
            "all files before split:  169389\n",
            "len(train_audio_files):  163671\n",
            "len(test_data):  5718\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def get_audio_files(root):\n",
        "    '''\n",
        "    root - root directory\n",
        "    '''\n",
        "    audio_files = []\n",
        "    for path, subdirs, files in os.walk(root):\n",
        "        for name in files:\n",
        "            if name.endswith('.wav'):\n",
        "                audio_files.append(os.path.join(path, name))\n",
        "    return audio_files\n",
        "\n",
        "def get_city_code(filenames):\n",
        "    '''\n",
        "    filenames - list of filenames\n",
        "    '''\n",
        "    city_codes = []\n",
        "    for audio_file in filenames:\n",
        "        base = os.path.basename(audio_file) # 70366_WG_WJ_404.2.wav\n",
        "        city = base.split('_')[0]\n",
        "        city_codes.append(city)\n",
        "    return city_codes\n",
        "\n",
        "root = '/home/projects/vokquant/data/dicla/augmented/'\n",
        "audio_files = get_audio_files(root)\n",
        "\n",
        "# read csv file with test speakers\n",
        "df_test_speakers = pd.read_csv('/home/projects/vokquant/data/dicla/speaker_test_set.csv', sep='\\t')\n",
        "# get the list of speaker ids\n",
        "speaker_ids = df_test_speakers['Sigle'].tolist()\n",
        "print(speaker_ids)\n",
        "\n",
        "# Remove Test data from the audio files\n",
        "print(\"all files before split: \", len(audio_files))\n",
        "train_audio_files = [audio_file for audio_file in audio_files if not any(speaker_id in audio_file for speaker_id in speaker_ids)]\n",
        "test_data = [audio_file for audio_file in audio_files if any(speaker_id in audio_file for speaker_id in speaker_ids)]\n",
        "print(\"len(train_audio_files): \", len(train_audio_files))\n",
        "print(\"len(test_data): \", len(test_data))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unique_test_city_codes:  ['20321', '70402', '70419', '10702', '80227', '50304', '10925', '40605', '50621', '62144', '41102', '61032', '31401', '80109', '30860', '21002']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "test_city_codes = get_city_code(test_data)\n",
        "# get unique city codes\n",
        "unique_test_city_codes = list(set(test_city_codes))\n",
        "print(\"unique_test_city_codes: \", unique_test_city_codes)\n",
        "# Shuffle and split the audio files into train and test\n",
        "# random.shuffle(audio_files)\n",
        "# train_files = audio_files[:int(0.95 * len(audio_files))]\n",
        "# test_files = audio_files[int(0.95 * len(audio_files)):]\n",
        "train_audio_files = shuffle(train_audio_files, random_state=42)\n",
        "train_files = train_audio_files\n",
        "test_data = shuffle(test_data, random_state=42)\n",
        "test_files = test_data\n",
        "\n",
        "# Get the city codes for train and test files\n",
        "y_train = get_city_code(train_files)\n",
        "y_test = get_city_code(test_files)\n",
        "\n",
        "assert len(y_train) == len(train_files)\n",
        "assert len(y_test) == len(test_files)\n",
        "\n",
        "# Encode the city codes\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "\n",
        "# Split the train set into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_files, y_train, test_size=0.05, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(X_train) 155487\n",
            "len(X_val) 8184\n",
            "len(y_train) 155487\n",
            "len(y_val) 8184\n",
            "len(test_files) 5718\n",
            "len(y_test) 5718\n",
            "len(le.classes_) 109\n",
            "le.classes_ ['10401' '10428' '10612' '10702' '10903' '10925' '20321' '20604' '20619'\n",
            " '20622' '20914' '21002' '30501' '30719' '30860' '30910' '31035' '31110'\n",
            " '31204' '31207' '31401' '31405' '31551' '31617' '31652' '31814' '31916'\n",
            " '32002' '32210' '32309' '32324' '32518' '32519' '40402' '40410' '40423'\n",
            " '40605' '40621' '40702' '40719' '40806' '40914' '41102' '41342' '41411'\n",
            " '41501' '41706' '41804' '50206' '50210' '50212' '50304' '50413' '50423'\n",
            " '50502' '50506' '50509' '50612' '50617' '50618' '50621' '50626' '60350'\n",
            " '61032' '61115' '61251' '61254' '61257' '61627' '61628' '61743' '61756'\n",
            " '62105' '62135' '62144' '62216' '62390' '70208' '70217' '70221' '70326'\n",
            " '70334' '70362' '70366' '70402' '70406' '70419' '70504' '70516' '70606'\n",
            " '70615' '70622' '70627' '70706' '70709' '70734' '70804' '70824' '70825'\n",
            " '70908' '70920' '80105' '80109' '80128' '80212' '80225' '80227' '80239'\n",
            " '80411']\n"
          ]
        }
      ],
      "source": [
        "print(\"len(X_train)\", len(X_train))\n",
        "print(\"len(X_val)\", len(X_val))\n",
        "print(\"len(y_train)\", len(y_train))\n",
        "print(\"len(y_val)\", len(y_val))\n",
        "print(\"len(test_files)\", len(test_files))\n",
        "print(\"len(y_test)\", len(y_test))\n",
        "print(\"len(le.classes_)\", len(le.classes_))\n",
        "\n",
        "# check if city codes in test files are in the train files\n",
        "for city_code in unique_test_city_codes:\n",
        "    if city_code in le.classes_:\n",
        "        continue\n",
        "    else:\n",
        "        print(f\"Warning: {city_code} is not in the train set\")\n",
        "        \n",
        "\n",
        "print(\"le.classes_\", le.classes_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 155487/155487 [00:12<00:00, 12819.03it/s]\n",
            "100%|██████████| 8184/8184 [00:00<00:00, 12990.34it/s]\n",
            "100%|██████████| 5718/5718 [00:00<00:00, 13069.47it/s]\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 12971.97it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 12140.86it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 12298.93it/s]\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "\n",
        "def create_csv_file(file_path, X, y):\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.write('ID,utt_id,wav,wav_format,text,duration,accent\\n')\n",
        "        for i in tqdm(range(len(X))):\n",
        "            basename = os.path.basename(X[i])\n",
        "            basename = basename.split('.wav')[0]\n",
        "            label = y[i]\n",
        "            label = le.inverse_transform([label])[0]\n",
        "            duration = librosa.get_duration(path=X[i])\n",
        "            duration = round(duration, 3)\n",
        "            f.write(f'{i},{basename},{X[i]},wav,,{duration},{label}\\n')\n",
        "\n",
        "os.makedirs('/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented', exist_ok=True)\n",
        "create_csv_file('/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented/train_regions.csv', X_train, y_train)\n",
        "create_csv_file('/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented/dev_regions.csv', X_val, y_val)\n",
        "create_csv_file('/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented/test_regions.csv', test_files, y_test)\n",
        "\n",
        "#create devolpment set\n",
        "os.makedirs('/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented_dev', exist_ok=True)\n",
        "create_csv_file('/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented_dev/train_regions.csv', X_train[:1000], y_train[:1000])\n",
        "create_csv_file('/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented_dev/dev_regions.csv', X_val[:100], y_val[:100])\n",
        "create_csv_file('/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented_dev/test_regions.csv', test_files[:100], y_test[:100])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split into 10 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "155488\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/155487 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 155487/155487 [00:00<00:00, 2136554.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8185\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8184/8184 [00:00<00:00, 2325307.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5719\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5718/5718 [00:00<00:00, 2377853.49it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def add_offset_for_long_audios(csv_path, max_audio_duration, make_state_level=True):\n",
        "    with open(csv_path, 'r') as f:\n",
        "        # if files longer than 10 seconds, then split them into 10 seconds\n",
        "        lines = f.readlines()\n",
        "        print(len(lines))\n",
        "        index = 0\n",
        "        index_list = []\n",
        "        lines_to_add = []\n",
        "        line_count = 0\n",
        "        lines_to_add.append('ID,utt_id,wav,wav_format,text,duration,offset,accent\\n')\n",
        "        make_state_level = make_state_level\n",
        "        for line in tqdm(lines[1:]):\n",
        "            # print(\"line: \", line)\n",
        "            line_count += 1\n",
        "            line = line.strip()\n",
        "            line = line.split(',')\n",
        "            wav_file = line[2]\n",
        "            duration = float(line[5])\n",
        "            # # number 1 is for Burgenland, 2 for Kärnten, 3 for Niederösterreich, 4 for Oberösterreich, 5 for Salzburg, 6 for Steiermark, 7 for Tirol, 8 for Vorarlberg\n",
        "            if make_state_level==True:\n",
        "                if line[6][0] == '1':    # check first number of line[6]\n",
        "                    line[6] = 'bgld'\n",
        "                elif line[6][0] == '2':\n",
        "                    line[6] = 'ktn'\n",
        "                elif line[6][0] == '3':\n",
        "                    line[6] = 'noe'\n",
        "                elif line[6][0] == '4':\n",
        "                    line[6] = 'ooe'\n",
        "                elif line[6][0] == '5':\n",
        "                    line[6] = 'sbg'\n",
        "                elif line[6][0] == '6':\n",
        "                    line[6] = 'stmk'\n",
        "                elif line[6][0] == '7':\n",
        "                    line[6] = 't'\n",
        "                elif line[6][0] == '8':\n",
        "                    line[6] = 'vbg'\n",
        "                elif line[6][0] == '9':\n",
        "                    line[6] = 'w'\n",
        "                else:\n",
        "                    print(\"Error: No state found\")\n",
        "                    break                  \n",
        "                \n",
        "            # print(f\"Duration: {duration} for {wav_file}\")\n",
        "            if duration > max_audio_duration:\n",
        "                split_files = []\n",
        "                for i in range(0, int(duration), max_audio_duration):\n",
        "                    start = i   # = offset_time\n",
        "                    end = i + max_audio_duration\n",
        "                    # split_file = wav_file.split('.wav')[0] + f'_{start}_{end}.wav'\n",
        "                    offset_time = start\n",
        "                    ##\n",
        "                    if duration-start>10:\n",
        "                        # print(\"duration: 10\")\n",
        "                        duration_chunk = max_audio_duration\n",
        "                    else:\n",
        "                        # print(f\"duration: {duration-start}\")\n",
        "                        duration_chunk = np.round(duration-start, 3)\n",
        "                    ##\n",
        "                    # if longer than max_audio then copy name and add offset to offset_column\n",
        "                    offset_time = start\n",
        "                    lines_to_add.append(f'{line[0]}, {line[1]}, {line[2]}, {line[3]}, {line[4]}, {duration_chunk}, {offset_time}, {line[6]}\\n')\n",
        "                # handle the last part of the audio file if is less than 1 second\n",
        "                if duration % max_audio_duration != 0 and duration % max_audio_duration >= 0.5 and duration % max_audio_duration < 1:   # only add for durations like 10.6 otherwise it is covered by above. Everything where int(duration) % 10 >= 1 is covered by above. So this is only for 0.5 seconds segments after 10, 20, 30, ...\n",
        "                    # this will use durations 0.5 to 1 seconds\n",
        "                    set_min_duration = 1\n",
        "                    # if set_min_duration >= 1  we do not need this part and can continue\n",
        "                    # this part was added to handle the case where the last part of the audio file is less than 1 second\n",
        "                    # but i do not need this anymore, as i only want audios with more than 1 second\n",
        "                    if set_min_duration < 1:\n",
        "                        continue\n",
        "                    else:\n",
        "                        start = int(duration) - (int(duration) % max_audio_duration)\n",
        "                        if duration - start >= 0.5:\n",
        "                            duration_chunk = np.round(duration-start, 3)\n",
        "                            offset_time = start\n",
        "                            lines_to_add.append(f'{line[0]}, {line[1]}, {line[2]}, {line[3]}, {line[4]}, {duration_chunk}, {offset_time}, {line[6]}\\n')\n",
        "                        else:\n",
        "                            # print(\"Last part of the audio file is less than 0.5 seconds\")\n",
        "                            continue\n",
        "            else:\n",
        "                # leave everything as it is and add an offset of 0\n",
        "                lines_to_add.append(f'{line[0]}, {line[1]}, {line[2]}, {line[3]}, {line[4]}, {line[5]}, 0, {line[6]}\\n')\n",
        "                \n",
        "    # Separate the header from the data\n",
        "    header = lines_to_add[0]\n",
        "    data_lines = lines_to_add[1:]\n",
        "    # check for duration < 0.95\n",
        "    data_lines_new = []\n",
        "    for line in data_lines:\n",
        "        parts = line.split(',')\n",
        "        # print(float(parts[5]))\n",
        "        if float(parts[5]) < 0.95:\n",
        "            # print(f\"Error: Duration is less than 0.95 seconds for {parts}\")\n",
        "            continue\n",
        "        else:\n",
        "            data_lines_new.append(line)\n",
        "    data_lines = data_lines_new\n",
        "    # Reset the ID column, starting from 0\n",
        "    new_data_lines = []\n",
        "    for i, line in enumerate(data_lines):\n",
        "        parts = line.split(',')\n",
        "        new_id = i  # IDs start from 0\n",
        "        new_line = f'{new_id},{\",\".join(parts[1:])}'\n",
        "        new_data_lines.append(new_line)\n",
        "        \n",
        "    # Combine header with new data lines\n",
        "    updated_lines_to_add = [header] + new_data_lines\n",
        "    \n",
        "    # remove double blank spaces\n",
        "    updated_lines_to_add = [re.sub(' +', '', line) for line in updated_lines_to_add]  # plus means one or more spaces\n",
        "\n",
        "    return updated_lines_to_add\n",
        "\n",
        "# Write the updated data to a new file\n",
        "def write_csv_file(file_path, lines_to_add):\n",
        "    with open(file_path, 'w') as f:\n",
        "        for line in lines_to_add:\n",
        "            f.write(line)\n",
        "# Use the function\n",
        "split_duration = 10\n",
        "make_state_level = False\n",
        "copy_files_inplace = True\n",
        "\n",
        "if make_state_level==True:\n",
        "    save_dir = '/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    updated_lines_to_add_train = add_offset_for_long_audios(os.path.join(save_dir, 'train_regions.csv'), split_duration, make_state_level=True)\n",
        "    updated_lines_to_add_dev = add_offset_for_long_audios(os.path.join(save_dir, 'dev_regions.csv'), split_duration, make_state_level=True)\n",
        "    updated_lines_to_add_test = add_offset_for_long_audios(os.path.join(save_dir, 'test_regions.csv'), split_duration, make_state_level=True)\n",
        "    # save to file\n",
        "    write_csv_file(f'{save_dir}/train_augmented_offset.csv', updated_lines_to_add_train)\n",
        "    write_csv_file(f'{save_dir}/dev_augmented_offset.csv', updated_lines_to_add_dev)\n",
        "    write_csv_file(f'{save_dir}/test_augmented_offset.csv', updated_lines_to_add_test)\n",
        "    if copy_files_inplace==True:\n",
        "        write_csv_file(f'{save_dir}/train.csv', updated_lines_to_add_train)\n",
        "        write_csv_file(f'{save_dir}/dev.csv', updated_lines_to_add_dev)\n",
        "        write_csv_file(f'{save_dir}/test.csv', updated_lines_to_add_test)\n",
        "elif make_state_level==False:\n",
        "    save_dir = '/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented_regions'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    updated_lines_to_add_train = add_offset_for_long_audios(os.path.join(save_dir, 'train_regions.csv'), split_duration, make_state_level=False)\n",
        "    updated_lines_to_add_dev = add_offset_for_long_audios(os.path.join(save_dir, 'dev_regions.csv'), split_duration, make_state_level=False)\n",
        "    updated_lines_to_add_test = add_offset_for_long_audios(os.path.join(save_dir, 'test_regions.csv'), split_duration, make_state_level=False)\n",
        "    # save to file\n",
        "    write_csv_file(f'{save_dir}/train_augmented_state_offset.csv', updated_lines_to_add_train)\n",
        "    write_csv_file(f'{save_dir}/dev_augmented_state_offset.csv', updated_lines_to_add_dev)\n",
        "    write_csv_file(f'{save_dir}/test_augmented_state_offset.csv', updated_lines_to_add_test)\n",
        "    if copy_files_inplace==True:\n",
        "        write_csv_file(f'{save_dir}/train.csv', updated_lines_to_add_train)\n",
        "        write_csv_file(f'{save_dir}/dev.csv', updated_lines_to_add_dev)\n",
        "        write_csv_file(f'{save_dir}/test.csv', updated_lines_to_add_test)\n",
        "\n",
        "# with open(f'{save_dir}/train_augmented_offset.csv', 'w') as f:\n",
        "#     for line in updated_lines_to_add_train:\n",
        "#         f.write(line)\n",
        "# with open(f'{save_dir}/dev_augmented_offset.csv', 'w') as f:\n",
        "#     for line in updated_lines_to_add_dev:\n",
        "#         f.write(line)\n",
        "# with open(f'{save_dir}/test_augmented_offset.csv', 'w') as f:\n",
        "#     for line in updated_lines_to_add_test:\n",
        "#         f.write(line)\n",
        "# copy_files_inplace = False\n",
        "# if copy_files_inplace == True:\n",
        "#     shutil.copyfile('/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented/train_augmented_offset.csv', '/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented/train.csv')\n",
        "#     shutil.copyfile('/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented/dev_augmented_offset.csv', '/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented/dev.csv')\n",
        "#     shutil.copyfile('/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented/test_augmented_offset.csv', '/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented/test.csv')    \n",
        "    \n",
        "# create devolpment set\n",
        "updated_lines_to_add_train_dev = updated_lines_to_add_train[:1000]\n",
        "updated_lines_to_add_dev_dev = updated_lines_to_add_dev[:100]\n",
        "updated_lines_to_add_test_dev = updated_lines_to_add_test[:100]\n",
        "\n",
        "# Write the updated data to a new file\n",
        "if make_state_level==True:\n",
        "    save_dir_dev = '/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented_dev'\n",
        "    os.makedirs(save_dir_dev, exist_ok=True)\n",
        "    write_csv_file(f'{save_dir_dev}/train_augmented_offset.csv', updated_lines_to_add_train_dev)\n",
        "    write_csv_file(f'{save_dir_dev}/dev_augmented_offset.csv', updated_lines_to_add_dev_dev)\n",
        "    write_csv_file(f'{save_dir_dev}/test_augmented_offset.csv', updated_lines_to_add_test_dev)\n",
        "elif make_state_level==False:\n",
        "    save_dir_dev = '/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented_dev_regions'\n",
        "    os.makedirs(save_dir_dev, exist_ok=True)\n",
        "    write_csv_file(f'{save_dir_dev}/train_augmented_regions_offset.csv', updated_lines_to_add_train_dev)\n",
        "    write_csv_file(f'{save_dir_dev}/dev_augmented_regions_offset.csv', updated_lines_to_add_dev_dev)\n",
        "    write_csv_file(f'{save_dir_dev}/test_augmented_regions_offset.csv', updated_lines_to_add_test_dev)\n",
        "\n",
        "# with open(f'{save_dir_dev}/train_augmented_offset.csv', 'w') as f:\n",
        "#     for line in updated_lines_to_add_train_dev:\n",
        "#         f.write(line)\n",
        "# with open(f'{save_dir_dev}/dev_augmented_offset.csv', 'w') as f:\n",
        "#     for line in updated_lines_to_add_dev_dev:\n",
        "#         f.write(line)\n",
        "# with open(f'{save_dir_dev}/test_augmented_offset.csv', 'w') as f:\n",
        "#     for line in updated_lines_to_add_test_dev:\n",
        "#         f.write(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Zip with pigz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tar: Removing leading `/' from member names\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tar -cf - /home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented | pigz -1 > /home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented/at_augmented.tar.gz\n",
        "# do with os.system\n",
        "os.system('tar -cf - /home/projects/vokquant/data/dicla/augmented/ | pigz -1 > /home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented/at_augmented_mono.tar.gz')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Make Files Mono"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torchaudio\n",
        "import librosa\n",
        "\n",
        "# BE CAREFUL: Librosa always outputs only one channel, while torchaudio outputs the number of channels in the audio file\n",
        "\n",
        "# audio = '/home/projects/vokquant/data/dicla/augmented/TS_WA/80128_TS_WA_454.01.wav'\n",
        "# audio_librosa, sr = librosa.load(audio, sr=None)\n",
        "# print(audio_librosa.shape)\n",
        "# torchaudio_1 = torchaudio.load(audio)\n",
        "# print(torchaudio_1[0].shape)\n",
        "\n",
        "folder = '/home/projects/vokquant/data/dicla/augmented/'\n",
        "count = 0\n",
        "for path, subdirs, files in os.walk(folder):\n",
        "    for name in files:\n",
        "        if name.endswith('.wav'):\n",
        "            audio_file = os.path.join(path, name)\n",
        "            audio, sr = torchaudio.load(audio_file)\n",
        "            if sr != 16000:\n",
        "                print(f\"{audio_file} has sample rate {sr}\")\n",
        "                continue\n",
        "            if len(audio.shape) > 1:\n",
        "                # print(f\"Converting {audio_file} to mono\")\n",
        "                audio = audio.mean(dim=0, keepdim=True)\n",
        "                safe_path = os.path.join(path, name)\n",
        "                # print(f\"Saving to {safe_path}\")\n",
        "                torchaudio.save(safe_path, audio, sr)\n",
        "                # count += 1\n",
        "            else:\n",
        "                continue\n",
        "                # print(f\"{audio_file} is already mono\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size in GB:  10.543154884129763\n"
          ]
        }
      ],
      "source": [
        "# get file size in GB for 'home/projects/vokquant/data/dicla/augmented/'\n",
        "def get_size(start_path = '.'):\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            total_size += os.path.getsize(fp)\n",
        "    return total_size\n",
        "\n",
        "size = get_size('/home/projects/vokquant/data/dicla/augmented/')\n",
        "print(\"size in GB: \", size / (1024**3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# generate MP3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            " 40 8068M    0     0   40 3264M      0  5425k  0:25:22  0:10:16  0:15:06 5034k"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<html>\n",
            "<head><title>500 Internal Server Error</title></head>\n",
            "<body bgcolor=\"white\">\n",
            "<center><h1>500 Internal Server Error</h1></center>\n",
            "<hr><center>nginx</center>\n",
            "</body>\n",
            "</html>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40 8068M    0   186   40 3266M      0  5425k  0:25:22  0:10:16  0:15:06 5188k\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "# upload to cloud\n",
        "# curl bashupload.com -T /home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented/at_augmented.tar.gz\n",
        "os.system('curl bashupload.com -T /home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_augmented/at_augmented.tar.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make to mp3\n",
        "\n",
        "folder = '/home/projects/vokquant/data/dicla/augmented/'\n",
        "count = 0\n",
        "for path, subdirs, files in os.walk(folder):\n",
        "    for name in files:\n",
        "        if name.endswith('.wav'):\n",
        "            audio_file = os.path.join(path, name)\n",
        "            audio, sr = torchaudio.load(audio_file)\n",
        "            # safe as mp3\n",
        "            # convert\n",
        "            mp3_file = audio_file.replace('.wav', '.mp3')\n",
        "            new_safe_path = audio_file.replace('augmented', 'augmented_mp3')\n",
        "            print(f\"New safe path: {new_safe_path}\")\n",
        "            # torchaudio.save(mp3_file, audio, sr, format='mp3')\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(30407,)\n",
            "torch.Size([1, 30407])\n"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dicla_torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
