{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split into 10 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "def add_offset_for_long_audios(csv_path, max_audio_duration):\n",
        "    with open(csv_path, 'r') as f:\n",
        "        # if files longer than 10 seconds, then split them into 10 seconds\n",
        "        lines = f.readlines()\n",
        "        print(len(lines))\n",
        "        index = 0\n",
        "        index_list = []\n",
        "        lines_to_add = []\n",
        "        line_count = 0\n",
        "        lines_to_add.append('ID,utt_id,wav,wav_format,text,duration,offset,accent\\n')\n",
        "        make_state_level = True\n",
        "        for line in tqdm(lines[1:]):\n",
        "            # print(\"line: \", line)\n",
        "            line_count += 1\n",
        "            line = line.strip()\n",
        "            line = line.split(',')\n",
        "            wav_file = line[2]\n",
        "            duration = float(line[5])\n",
        "            if make_state_level==True:\n",
        "                if line[6][0] == '1':    # check first number of line[6]\n",
        "                    line[6] = '1'\n",
        "                elif line[6][0] == '2':\n",
        "                    line[6] = '2'\n",
        "                elif line[6][0] == '3':\n",
        "                    line[6] = '3'\n",
        "                elif line[6][0] == '4':\n",
        "                    line[6] = '4'\n",
        "                elif line[6][0] == '5':\n",
        "                    line[6] = '5'\n",
        "                elif line[6][0] == '6':\n",
        "                    line[6] = '6'\n",
        "                elif line[6][0] == '7':\n",
        "                    line[6] = '7'\n",
        "                elif line[6][0] == '8':\n",
        "                    line[6] = '8'                \n",
        "            # print(f\"Duration: {duration} for {wav_file}\")\n",
        "            if duration > max_audio_duration:\n",
        "                split_files = []\n",
        "                for i in range(0, int(duration), max_audio_duration):\n",
        "                    start = i   # = offset_time\n",
        "                    end = i + max_audio_duration\n",
        "                    # split_file = wav_file.split('.wav')[0] + f'_{start}_{end}.wav'\n",
        "                    offset_time = start\n",
        "                    ##\n",
        "                    if duration-start>10:\n",
        "                        # print(\"duration: 10\")\n",
        "                        duration_chunk = max_audio_duration\n",
        "                    else:\n",
        "                        # print(f\"duration: {duration-start}\")\n",
        "                        duration_chunk = np.round(duration-start, 3)\n",
        "                    ##\n",
        "                    # if longer than max_audio then copy name and add offset to offset_column\n",
        "                    offset_time = start\n",
        "                    lines_to_add.append(f'{line[0]}, {line[1]}, {line[2]}, {line[3]}, {line[4]}, {duration_chunk}, {offset_time}, {line[6]}\\n')\n",
        "                # handle the last part of the audio file\n",
        "                if duration % max_audio_duration != 0 and duration % max_audio_duration >= 0.5 and duration % max_audio_duration < 1:   # only add for durations like 10.6 otherwise it is covered by above. Everything where int(duration) % 10 >= 1 is covered by above. So this is only for 0.5 seconds segments after 10, 20, 30, ...\n",
        "                    start = int(duration) - (int(duration) % max_audio_duration)\n",
        "                    if duration - start >= 0.5:\n",
        "                        duration_chunk = np.round(duration-start, 3)\n",
        "                        offset_time = start\n",
        "                        lines_to_add.append(f'{line[0]}, {line[1]}, {line[2]}, {line[3]}, {line[4]}, {duration_chunk}, {offset_time}, {line[6]}\\n')\n",
        "                    else:\n",
        "                        # print(\"Last part of the audio file is less than 0.5 seconds\")\n",
        "                        continue\n",
        "            else:\n",
        "                # leave everything as it is and add an offset of 0\n",
        "                lines_to_add.append(f'{line[0]}, {line[1]}, {line[2]}, {line[3]}, {line[4]}, {line[5]}, 0, {line[6]}\\n')\n",
        "                \n",
        "    # Separate the header from the data\n",
        "    header = lines_to_add[0]\n",
        "    data_lines = lines_to_add[1:]\n",
        "    \n",
        "    # Reset the ID column, starting from 0\n",
        "    new_data_lines = []\n",
        "    for i, line in enumerate(data_lines):\n",
        "        parts = line.split(',')\n",
        "        new_id = i  # IDs start from 0\n",
        "        new_line = f'{new_id},{\",\".join(parts[1:])}'\n",
        "        new_data_lines.append(new_line)\n",
        "        \n",
        "    # Combine header with new data lines\n",
        "    updated_lines_to_add = [header] + new_data_lines\n",
        "    \n",
        "    # remove double blank spaces\n",
        "    updated_lines_to_add = [re.sub(' +', '', line) for line in updated_lines_to_add]  # plus means one or more spaces\n",
        "\n",
        "    return updated_lines_to_add\n",
        "\n",
        "# test the function\n",
        "split_duration = 10\n",
        "updated_lines_to_add_train = add_offset_for_long_audios('/nas/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at/train.csv', split_duration)\n",
        "updated_lines_to_add_dev = add_offset_for_long_audios('/nas/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at/dev.csv', split_duration)\n",
        "updated_lines_to_add_test = add_offset_for_long_audios('/nas/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at/test.csv', split_duration)\n",
        "\n",
        "# Write the updated data to a new file\n",
        "with open('/nas/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at/train_offset2.csv', 'w') as f:\n",
        "    for line in updated_lines_to_add_train:\n",
        "        f.write(line)\n",
        "\n",
        "with open('/nas/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at/dev_offset2.csv', 'w') as f:\n",
        "    for line in updated_lines_to_add_dev:\n",
        "        f.write(line)\n",
        "        \n",
        "with open('/nas/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at/test_offset2.csv', 'w') as f:\n",
        "    for line in updated_lines_to_add_test:\n",
        "        f.write(line)\n",
        "\n",
        "# create devolpment set\n",
        "updated_lines_to_add_train_dev = updated_lines_to_add_train[:1000]\n",
        "updated_lines_to_add_dev_dev = updated_lines_to_add_dev[:100]\n",
        "updated_lines_to_add_test_dev = updated_lines_to_add_test[:100]\n",
        "\n",
        "# Write the updated data to a new file\n",
        "with open('/nas/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_dev/train_offset2.csv', 'w') as f:\n",
        "    for line in updated_lines_to_add_train_dev:\n",
        "        f.write(line)\n",
        "\n",
        "with open('/nas/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_dev/dev_offset2.csv', 'w') as f:\n",
        "    for line in updated_lines_to_add_dev_dev:\n",
        "        f.write(line)\n",
        "\n",
        "with open('/nas/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_dev/test_offset2.csv', 'w') as f:\n",
        "    for line in updated_lines_to_add_test_dev:\n",
        "        f.write(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223735/223735 [00:00<00:00, 3178323.09it/s]\n",
            "100%|██████████| 11779/11779 [00:00<00:00, 3176130.30it/s]\n",
            "100%|██████████| 7381/7381 [00:00<00:00, 3005062.88it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "def minimum_duration(csv_path, min_audio_duration):\n",
        "    with open(csv_path, 'r') as f:\n",
        "        # if files longer than 10 seconds, then split them into 10 seconds\n",
        "        lines = f.readlines()\n",
        "        # print(len(lines))\n",
        "        lines_to_add = []\n",
        "        lines_to_add.append('ID,utt_id,wav,wav_format,text,duration,offset,accent\\n')\n",
        "        for line in tqdm(lines[1:]):\n",
        "            line = line.strip()\n",
        "            line = line.split(',')\n",
        "            if float(line[5]) >= float(min_audio_duration):\n",
        "                lines_to_add.append(f'{line[0]},{line[1]},{line[2]},{line[3]},{line[4]},{line[5]},{line[6]},{line[7]}\\n')\n",
        "            else:\n",
        "                # nothing is added\n",
        "                continue\n",
        "    return lines_to_add\n",
        "\n",
        "# test the function\n",
        "min_duration = 2\n",
        "train_csv = '/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_states/train_all.csv'\n",
        "updated_lines_to_add_train = minimum_duration(train_csv, min_duration)\n",
        "dev_csv = '/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_states/dev_all.csv'\n",
        "updated_lines_to_add_dev = minimum_duration(dev_csv, min_duration)\n",
        "test_csv = '/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_states/test_all.csv'\n",
        "updated_lines_to_add_test = minimum_duration(test_csv, min_duration)\n",
        "\n",
        "# Write the updated data to a new file\n",
        "with open('/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_states/train_mindur2.csv', 'w') as f:\n",
        "    for line in updated_lines_to_add_train:\n",
        "        f.write(line)\n",
        "\n",
        "with open('/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_states/dev_mindur2.csv', 'w') as f:\n",
        "    for line in updated_lines_to_add_dev:\n",
        "        f.write(line)\n",
        "        \n",
        "with open('/home/projects/vokquant/accent-recog-slt2022/CommonAccent/data/at_states/test_mindur2.csv', 'w') as f:\n",
        "    for line in updated_lines_to_add_test:\n",
        "        f.write(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Read classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "precision         0.615\n",
            "recall         0.596667\n",
            "f1_score       0.573333\n",
            "support      104.166667\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "with open('/nas/projects/vokquant/accent-recog-slt2022/CommonAccent/results/ECAPA-TDNN/AT/spkrec-ecapa-voxceleb/10000/classification_report_test.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    accent_dict = {}\n",
        "    for line in lines[2:]:\n",
        "        if line.strip() == \"\":\n",
        "            break\n",
        "        split_line = line.split()\n",
        "        city_code = split_line[0]\n",
        "        precision = split_line[1]\n",
        "        recall = split_line[2]\n",
        "        f1_score = split_line[3]\n",
        "        support = split_line[4]\n",
        "        accent_dict[city_code] = [precision, recall, f1_score, support]\n",
        "        \n",
        "accent_df = pd.DataFrame.from_dict(accent_dict, orient='index', columns=['precision', 'recall', 'f1_score', 'support'])\n",
        "# name index to city_code\n",
        "accent_df.index.name = 'city_code'\n",
        "accent_df\n",
        "# convert precision, recall, f1_score, and support to float\n",
        "accent_df.loc[:, 'precision'] = accent_df['precision'].astype(float)\n",
        "accent_df.loc[:, 'recall'] = accent_df['recall'].astype(float)\n",
        "accent_df.loc[:, 'f1_score'] = accent_df['f1_score'].astype(float)\n",
        "accent_df.loc[:, 'support'] = accent_df['support'].astype(float)\n",
        "# find all city codes starting with a 1\n",
        "region_1 = accent_df[accent_df.index.str.startswith('1')]\n",
        "rounded_means = region_1.mean().round(3)\n",
        "# round to three decimals\n",
        "print(rounded_means)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
